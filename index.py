import asyncio
import logging
from aiogram import Router, types, F, Bot
from aiogram.fsm.context import FSMContext
from aiogram.fsm.state import State, StatesGroup
from aiogram.filters.command import Command
from aiogram.types import InlineKeyboardButton, InlineKeyboardMarkup, Message, CallbackQuery
from sqlalchemy.ext.asyncio import async_session, AsyncSession
from aiogram.fsm.storage.memory import MemoryStorage
from aiogram.dispatcher.dispatcher import Dispatcher
from DB import User, History, create_tables as init_db, async_session as async_session_maker, create_history, get_user_history
from aiogram.dispatcher.middlewares.base import BaseMiddleware
from typing import Any, Awaitable, Callable, Dict
from sqlalchemy import select
import os
from chat_pdf import PDFChat
from dotenv import load_dotenv
import json
from langchain.graphs import NetworkxEntityGraph
from langchain.graphs.graph_store import GraphStore
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain.chains import GraphQAChain
from langchain.chat_models import ChatOpenAI
from Hackaton.legal_qa import LegalQASystem
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage

# –°–æ–∑–¥–∞–µ–º —Ä–æ—É—Ç–µ—Ä
router = Router()


# –ö–ª–∞—Å—Å –¥–ª—è —Å–æ—Å—Ç–æ—è–Ω–∏–π –±–æ—Ç–∞
class BotStates(StatesGroup):
    waiting_for_name = State()
    waiting_for_file = State()  # New state for file upload
    analytics_mode = State()
    chat_tz = State()


# –•—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ (–≤ —Ä–µ–∞–ª—å–Ω–æ–º –ø—Ä–æ–µ–∫—Ç–µ –ª—É—á—à–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö)
analyses = {}


# –ö–ª–∞–≤–∏–∞—Ç—É—Ä—ã
def get_analytics_keyboard():
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="–ü–æ–ª—É—á–∏—Ç—å —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –ù–ü–ê", callback_data="get_recommendations")
        ],
        [
            InlineKeyboardButton(text="–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ –¢–ó", callback_data="analyze_tz_text")
        ],
        [
            InlineKeyboardButton(text="–í—ã—Ö–æ–¥ –∏–∑ —Ä–µ–∂–∏–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏", callback_data="exit_analytics"),
            InlineKeyboardButton(text="–ß–∞—Ç –ø–æ –¢–ó", callback_data="chat_tz")
        ]
    ])
    return keyboard


def get_history_keyboard():
    buttons = []
    for analysis_id, analysis in analyses.items():
        buttons.append([InlineKeyboardButton(
            text=f"–ê–Ω–∞–ª–∏–∑ {analysis_id}",
            callback_data=f"analysis_{analysis_id}"
        )])
    return InlineKeyboardMarkup(inline_keyboard=buttons)


def get_analysis_options_keyboard():
    keyboard = InlineKeyboardMarkup(inline_keyboard=[
        [
            InlineKeyboardButton(text="–†–µ–∂–∏–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∏", callback_data="start_analytics"),
            InlineKeyboardButton(text="–£–¥–∞–ª–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞", callback_data="delete_analysis"),
            InlineKeyboardButton(text="–ù–∞–∑–∞–¥", callback_data="back_to_history")
        ]
    ])
    return keyboard


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ –∫–æ–º–∞–Ω–¥


@router.message(Command("start"))
@router.message(F.text.lower() == "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è")
async def register_user(message: Message, session: AsyncSession):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    user_id = message.from_user.id
    username = message.from_user.full_name

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–∂–µ –≤ –±–∞–∑–µ –ø–æ users_id
        existing_user = await session.execute(select(User).where(User.users_id == user_id))
        existing_user = existing_user.scalar_one_or_none()
        
        if existing_user:
            await message.answer("üîÑ –í—ã —É–∂–µ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω—ã!")
            return

        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        new_user = User(
            users_id=user_id,
            users_name=username
        )

        session.add(new_user)
        await session.commit()

        await message.answer(
            "‚úÖ –†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞!\n"
            f"ID: {user_id}\n"
            f"–ò–º—è: {username}"
        )

    except Exception as e:
        await session.rollback()
        await message.answer("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
        print(f"–û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏: {e}")


@router.message(BotStates.waiting_for_name)
async def process_name(message: types.Message, state: FSMContext, session: AsyncSession):
    user_name = message.text
    user_id = message.from_user.id

    if len(user_name) < 2:
        await message.reply("–ò–º—è –¥–æ–ª–∂–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã 2 —Å–∏–º–≤–æ–ª–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑:")
        return

    # –ó–¥–µ—Å—å –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –ø–æ–ª–µ –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –∏–∑ –º–æ–¥–µ–ª–∏ User
    new_user = User(
        users_id=user_id,  # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –ø–æ–ª—è –∏–∑ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
        users_name=user_name
    )
    session.add(new_user)
    await session.commit()

    await message.reply(
        f"–û—Ç–ª–∏—á–Ω–æ, {user_name}! –î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å! –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /help –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –∫–æ–º–∞–Ω–¥."
    )
    await state.clear()


@router.message(Command("help"))
async def cmd_help(message: types.Message):
    help_text = """
    ü§ñ –û–ø–∏—Å–∞–Ω–∏–µ —Ä–∞–±–æ—Ç—ã —Å–µ—Ä–≤–∏—Å–∞:

    /upload_tech - –ó–∞–ø—É—Å–∫ —Ä–µ–∂–∏–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
    /history - –ü—Ä–æ—Å–º–æ—Ç—Ä –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –∞–Ω–∞–ª–∏–∑–æ–≤
    /help - –ü–æ–∫–∞–∑–∞—Ç—å —ç—Ç–æ —Å–æ–æ–±—â–µ–Ω–∏–µ

    –í —Ä–µ–∂–∏–º–µ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –≤—ã –º–æ–∂–µ—Ç–µ:
    - –û–±—â–∞—Ç—å—Å—è —Å –±–æ—Ç–æ–º –ø–æ –¢–ó
    - –í—ã–π—Ç–∏ –∏–∑ —Ä–µ–∂–∏–º–∞ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏

    –í –∏—Å—Ç–æ—Ä–∏–∏ –∞–Ω–∞–ª–∏–∑–æ–≤ –≤—ã –º–æ–∂–µ—Ç–µ:
    - –ü—Ä–æ—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å –ø—Ä–æ—à–ª—ã–µ –∞–Ω–∞–ª–∏–∑—ã
    - –£–¥–∞–ª—è—Ç—å –∞–Ω–∞–ª–∏–∑—ã
    - –ó–∞–ø—É—Å–∫–∞—Ç—å —Ä–µ–∂–∏–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
    """
    await message.reply(help_text)


@router.message(Command("upload_tech"))
async def cmd_upload_tech(message: Message, state: FSMContext):
    await state.set_state(BotStates.waiting_for_file)
    await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF –∏–ª–∏ DOCX.")


@router.message(BotStates.waiting_for_file)
async def process_tech_doc(message: Message, state: FSMContext, session: AsyncSession):
    if not message.document:
        await message.reply("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç (PDF –∏–ª–∏ DOCX —Ñ–∞–π–ª).")
        return

    file_name = message.document.file_name.lower()
    if not file_name.endswith(('.pdf', '.docx')):
        await message.reply("–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ PDF –∏–ª–∏ DOCX.")
        return

    thinking_message = await message.reply("–î—É–º–∞—é...")  # Notify the user about processing

    try:
        # Download the file
        file = await message.bot.get_file(message.document.file_id)
        downloaded_file = await message.bot.download_file(file.file_path)
        
        # Save the file locally
        local_file_path = f"temp_{message.document.file_id}{os.path.splitext(file_name)[1]}"
        with open(local_file_path, 'wb') as f:
            f.write(downloaded_file.read())
        ollama_host = 'http://192.168.1.18:11434'
        pdf_chat = PDFChat(ollama_host=ollama_host)
        
        # Load the document based on its type
        if file_name.endswith('.pdf'):
            pdf_chat.load_pdf(local_file_path)
        else:
            pdf_chat.load_docx(local_file_path)

        # Store PDFChat instance in state
        await state.update_data(pdf_chat=pdf_chat)
        
        # Create history record
        user_id = message.from_user.id
        history_record = await create_history(
            session=session,
            users_id=user_id,
            path_to_tz=local_file_path,
            path_to_analyze="pending"
        )
        await session.commit()
        
        # Clean up the temporary file
        os.remove(local_file_path)
        
        await thinking_message.delete()  # Remove '–î—É–º–∞—é...' after processing
        await message.reply(
            "‚úÖ –§–∞–π–ª —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω –∏ –æ–±—Ä–∞–±–æ—Ç–∞–Ω.\n–†–µ–∂–∏–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.",
            reply_markup=get_analytics_keyboard()
        )
    except Exception as e:
        await session.rollback()
        await thinking_message.delete()  # Remove '–î—É–º–∞—é...' in case of an error
        await message.reply("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        logging.error(f"File processing error: {str(e)}")
        await state.set_state(BotStates.waiting_for_file)
        if 'local_file_path' in locals():
            try:
                os.remove(local_file_path)
            except:
                pass


@router.message(Command("history"))
async def cmd_history(message: types.Message, session: AsyncSession):
    user_id = message.from_user.id
    
    try:
        # –ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        user_history = await get_user_history(session, users_id=user_id)
        
        if not user_history:
            await message.reply("–ò—Å—Ç–æ—Ä–∏—è –∞–Ω–∞–ª–∏–∑–æ–≤ –ø—É—Å—Ç–∞.")
            return

        # –°–æ–∑–¥–∞–µ–º –∫–ª–∞–≤–∏–∞—Ç—É—Ä—É —Å –∏—Å—Ç–æ—Ä–∏–µ–π
        buttons = []
        for history in user_history:
            buttons.append([InlineKeyboardButton(
                text=f"–ê–Ω–∞–ª–∏–∑ #{history.id}",
                callback_data=f"analysis_{history.id}"
            )])
        
        keyboard = InlineKeyboardMarkup(inline_keyboard=buttons)
        await message.reply("–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã:", reply_markup=keyboard)
        
    except Exception as e:
        await message.reply(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∏—Å—Ç–æ—Ä–∏–∏: {e}")


# –û–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ callback'–æ–≤
@router.callback_query(F.data == "chat_tz")
async def process_chat_tz(callback_query: types.CallbackQuery, state: FSMContext):
    await state.set_state(BotStates.chat_tz)
    await callback_query.message.reply("–†–µ–∂–∏–º —á–∞—Ç–∞ –ø–æ –¢–ó –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω. –ù–∞–ø–∏—à–∏—Ç–µ –≤–∞—à–µ —Å–æ–æ–±—â–µ–Ω–∏–µ.")
    await callback_query.answer()


@router.callback_query(F.data == "exit_analytics")
async def process_exit_analytics(callback_query: types.CallbackQuery, state: FSMContext):
    await state.clear()
    await callback_query.message.reply("–†–µ–∂–∏–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –¥–µ–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.")
    await callback_query.answer()


@router.callback_query(F.data.startswith("analysis_"))
async def process_analysis_selection(callback_query: types.CallbackQuery):
    analysis_id = callback_query.data.split("_")[1]
    await callback_query.message.reply(
        f"–í—ã–±—Ä–∞–Ω –∞–Ω–∞–ª–∏–∑ {analysis_id}",
        reply_markup=get_analysis_options_keyboard()
    )
    await callback_query.answer()


@router.callback_query(F.data == "start_analytics")
async def process_start_analytics(callback_query: types.CallbackQuery, state: FSMContext, session: AsyncSession):
    user_id = callback_query.from_user.id
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø–∏—Å—å –≤ –∏—Å—Ç–æ—Ä–∏–∏
        history_record = await create_history(
            session=session,
            users_id=user_id,
            path_to_tz="path/to/tz",  # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –¢–ó
            path_to_analyze="path/to/analysis"  # –ó–¥–µ—Å—å –Ω—É–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –∞–Ω–∞–ª–∏–∑—É
        )
        await session.commit()
        
        await state.set_state(BotStates.analytics_mode)
        await callback_query.message.reply(
            "–†–µ–∂–∏–º –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.",
            reply_markup=get_analytics_keyboard()
        )
    except Exception as e:
        await session.rollback()
        await callback_query.message.reply(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∑–∞–ø–∏—Å–∏ –≤ –∏—Å—Ç–æ—Ä–∏–∏: {e}")
    
    await callback_query.answer()


@router.callback_query(F.data == "delete_analysis")
async def process_delete_analysis(callback_query: types.CallbackQuery):
    analysis_id = callback_query.message.text.split()[-1]
    if analysis_id in analyses:
        del analyses[analysis_id]
        await callback_query.message.reply("–ê–Ω–∞–ª–∏–∑ —É–¥–∞–ª–µ–Ω.")
    await callback_query.message.reply(
        "–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã:",
        reply_markup=get_history_keyboard()
    )
    await callback_query.answer()


@router.callback_query(F.data == "back_to_history")
async def process_back_to_history(callback_query: types.CallbackQuery):
    await callback_query.message.reply(
        "–ü—Ä–µ–¥—ã–¥—É—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã:",
        reply_markup=get_history_keyboard()
    )
    await callback_query.answer()


@router.callback_query(F.data == "get_recommendations")
async def process_get_recommendations(callback_query: CallbackQuery, state: FSMContext):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ù–ü–ê –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –¢–ó
    """
    thinking_msg = await callback_query.message.answer("–î—É–º–∞—é...")
    
    try:
        # Get PDFChat instance and extracted text from state
        data = await state.get_data()
        pdf_chat = data.get('pdf_chat')
        
        if not pdf_chat:
            await thinking_msg.edit_text("‚ùå –û—à–∏–±–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ /upload_tech")
            return

        # First agent: Generate a focused legal question based on the technical specification
        query_prompt = """
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ –∑–∞–¥–∞–Ω–∏–µ –∏ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –≤ —Å–ª–µ–¥—É—é—â–µ–º —Ñ–æ—Ä–º–∞—Ç–µ:
        "–Ø —Ö–æ—á—É [–¥–µ–π—Å—Ç–≤–∏–µ/—Ü–µ–ª—å –∏–∑ –¢–ó] –≤ [–º–µ—Å—Ç–æ/—É—Å–ª–æ–≤–∏—è –∏–∑ –¢–ó], –∫–∞–∫–∏–µ –∑–∞–∫–æ–Ω—ã –∏ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ –∞–∫—Ç—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–æ–±–ª—é–¥–∞—Ç—å?"
        
        –ù–∞–ø—Ä–∏–º–µ—Ä: "–Ø —Ö–æ—á—É –æ—Ç–∫—Ä—ã—Ç—å –∫–æ—Ñ–µ–π–Ω—é –≤ –†–æ—Å—Ç–æ–≤–µ-–Ω–∞-–î–æ–Ω—É, –∫–∞–∫–∏–µ –∑–∞–∫–æ–Ω—ã –º–Ω–µ –Ω—É–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å?"
        """
        legal_question = pdf_chat.ask(query_prompt)
        
        # Initialize LegalQASystem
        qa_system = LegalQASystem()
        qa_system.load_documents("Hackaton\parsed_legal_documents.json")
        
        # Get recommendations using the generated legal question
        results = qa_system.answer_question(legal_question)
        
        if results.get("–æ—Ç–≤–µ—Ç—ã"):

                        
            # Format the response
            response = f"üîç –°—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–æ–ø—Ä–æ—Å:\n{legal_question}\n\n"
            response += "üìö –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ –∞–∫—Ç—ã:\n\n"
            for i, answer in enumerate(results["–æ—Ç–≤–µ—Ç—ã"], 1):
                             # Convert confidence to text format
                confidence_text = "–º–∞–ª–æ–≤–∞–∂–Ω–æ"
                if answer["—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] > 0.9:
                    confidence_text = "–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ"
                elif answer["—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] > 0.8:
                    confidence_text = "–≤–∞–∂–Ω–æ"
                response += f"{i}. {answer['–¥–æ–∫—É–º–µ–Ω—Ç']}\n"
                response +=     f"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {confidence_text}\n\n"
                response += f"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {answer['–æ—Ç–≤–µ—Ç']}\n\n"
        else:
            response = "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã—Ö –∞–∫—Ç–æ–≤."
        
        await thinking_msg.edit_text(response)
        
    except Exception as e:
        logging.error(f"Error getting NPA recommendations: {str(e)}")
        await thinking_msg.edit_text("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


@router.callback_query(F.data == "analyze_tz_text")
async def analyze_tz_text(callback_query: CallbackQuery, state: FSMContext):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Ç–µ–∫—Å—Ç –¢–ó –ø–æ –∞–±–∑–∞—Ü–∞–º –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –ù–ü–ê
    """
    thinking_msg = await callback_query.message.answer("–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é —Ç–µ–∫—Å—Ç –¢–ó...")
    
    try:
        # Get PDFChat instance from state
        data = await state.get_data()
        pdf_chat = data.get('pdf_chat')
        
        if not pdf_chat:
            await thinking_msg.edit_text("‚ùå –û—à–∏–±–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ /upload_tech")
            return

        # Initialize LegalQASystem
        qa_system = LegalQASystem()
        qa_system.load_documents("Hackaton/parsed_legal_documents.json")
        
        # Get the full text and split into sections and paragraphs
        full_text = pdf_chat.get_document_text()
        
        # Split text into sections by double newlines and handle potential section headers
        sections = []
        current_section = []
        
        for line in full_text.split('\n'):
            line = line.strip()
            if not line:
                if current_section:
                    sections.append('\n'.join(current_section))
                    current_section = []
            else:
                current_section.append(line)
        
        if current_section:
            sections.append('\n'.join(current_section))
        
        # Set to track unique NPAs used in analysis
        used_npas = set()
        
        # Build HTML response
        html_response = "<h2>–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è</h2>"
        
        # Analyze each section
        for section in sections:
            if not section.strip():
                continue
            
            # Check if the section starts with a number (likely a section header)
            lines = section.split('\n')
            first_line = lines[0]
            
            # If it's a section header, format it accordingly
            if any(char.isdigit() for char in first_line[:2]):
                html_response += f"<h3 class='section-header'>{first_line}</h3>"
                section_content = '\n'.join(lines[1:])
            else:
                section_content = section
            
            # Add section content
            if section_content.strip():
                html_response += f"<div class='section-content'>"
                html_response += f"<p class='paragraph'>{section_content}</p>"
                
                # Check for legal violations
                legal_question = f"–ü—Ä–æ–≤–µ—Ä—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –∑–∞–∫–æ–Ω–æ–¥–∞—Ç–µ–ª—å—Å—Ç–≤–∞: {section_content}"
                legal_results = qa_system.answer_question(legal_question)
                
                violations = []
                if legal_results.get("–æ—Ç–≤–µ—Ç—ã"):
                    for answer in legal_results["–æ—Ç–≤–µ—Ç—ã"]:
                        if answer["—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] > 0.7:
                            doc = answer["–¥–æ–∫—É–º–µ–Ω—Ç"]
                            chapter = answer.get("–≥–ª–∞–≤–∞", "")
                            article = answer.get("—Å—Ç–∞—Ç—å—è", "")
                            reference = f"{doc} {chapter} {article}".strip()
                            
                            used_npas.add(doc)
                            
                            # Convert confidence to text format
                            confidence_text = "–º–∞–ª–æ–≤–∞–∂–Ω–æ"
                            if answer["—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] > 0.9:
                                confidence_text = "–æ—á–µ–Ω—å –≤–∞–∂–Ω–æ"
                            elif answer["—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å"] > 0.8:
                                confidence_text = "–≤–∞–∂–Ω–æ"
                            
                            if "–Ω–∞—Ä—É—à–µ–Ω" in answer["–æ—Ç–≤–µ—Ç"].lower() or "—Ç—Ä–µ–±—É–µ—Ç" in answer["–æ—Ç–≤–µ—Ç"].lower():
                                violations.append(
                                    f"<div class='violation'>"
                                    f"<em><strong>‚ö†Ô∏è {answer['–æ—Ç–≤–µ—Ç']}</strong></em><br>"
                                    f"<span class='reference'>üìö {reference}</span><br>"
                                    f"<span class='relevance'>–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {confidence_text}</span>"
                                    f"</div>"
                                )
                
                if violations:
                    html_response += "\n".join(violations)
                
                # First add used NPAs before style recommendations
                if used_npas:
                    html_response += "<div class='used-npas'>"
                    html_response += "<h3>–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω–æ-–ø—Ä–∞–≤–æ–≤—ã–µ –∞–∫—Ç—ã:</h3>"
                    html_response += "<ul class='npa-list'>"
                    for npa in sorted(used_npas):
                        html_response += f"<li>{npa}</li>"
                    html_response += "</ul></div>"
                
                # Then check for stylistic issues
                style_question = f"–ü—Ä–æ–≤–µ—Ä—å —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏ –∏ –Ω–µ—Ç–æ—á–Ω–æ—Å—Ç–∏ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫: {section_content}"
                style_analysis = pdf_chat.ask(style_question)
                
                if style_analysis and ("–æ—à–∏–±–∫" in style_analysis.lower() or "–Ω–µ—Ç–æ—á–Ω–æ—Å—Ç" in style_analysis.lower()):
                    html_response += (
                        f"<div class='style-note'>"
                        f"<em>üí° –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:</em><br>"
                        f"{style_analysis}"
                        f"</div>"
                    )
                
                html_response += "</div>"
        
        # Get overall assessment of the document
        overall_assessment = pdf_chat.ask("""
        –î–∞–π –æ–±—â—É—é –æ—Ü–µ–Ω–∫—É —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–º—É –∑–∞–¥–∞–Ω–∏—é –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º:
        1. –ü–æ–ª–Ω–æ—Ç–∞ –∏ —á–µ—Ç–∫–æ—Å—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        3. –°—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∞—è –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å
        4. –†–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
        5. –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
        
        –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: –ø–æ –∫–∞–∂–¥–æ–º—É –ø—É–Ω–∫—Ç—É –¥–∞–π –∫—Ä–∞—Ç–∫—É—é –æ—Ü–µ–Ω–∫—É –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.
        """)
        
        # Add overall assessment section
        if overall_assessment:
            html_response += """
            <div class='overall-assessment'>
                <h3>–û–±—â–µ–µ –∑–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç—É</h3>
                <div class='assessment-content'>
                    """ + overall_assessment.replace('\n', '<br>') + """
                </div>
            </div>
            """
        
        # Save the HTML file
        output_path = "tz_analysis.html"
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="utf-8">
                <style>
                    body {{ 
                        font-family: Arial, sans-serif; 
                        margin: 40px;
                        line-height: 1.6;
                        max-width: 1200px;
                        margin-left: auto;
                        margin-right: auto;
                        color: #333;
                        background-color: #f9f9f9;
                    }}
                    h2 {{ 
                        color: #2c3e50;
                        margin-bottom: 40px;
                        padding-bottom: 15px;
                        border-bottom: 2px solid #eee;
                        text-align: center;
                    }}
                    h3 {{ 
                        color: #34495e; 
                        margin-top: 40px;
                        margin-bottom: 20px;
                    }}
                    .section-header {{
                        background-color: #edf2f7;
                        padding: 15px;
                        border-radius: 6px;
                        margin-top: 40px;
                        font-weight: bold;
                        color: #2d3748;
                    }}
                    .section-content {{
                        margin: 25px 0;
                        padding: 0 20px;
                    }}
                    .paragraph {{
                        white-space: pre-line;
                        background-color: white;
                        padding: 25px;
                        margin: 20px 0;
                        border-radius: 8px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
                        border-left: 4px solid #95a5a6;
                    }}
                    .violation {{ 
                        background-color: #fff5f5;
                        padding: 20px;
                        margin: 15px 0;
                        border-radius: 8px;
                        border-left: 4px solid #fc8181;
                    }}
                    .violation strong {{
                        color: #c53030;
                    }}
                    .reference {{
                        display: block;
                        margin-top: 10px;
                        color: #4a5568;
                        font-size: 0.9em;
                    }}
                    .relevance {{
                        display: block;
                        margin-top: 5px;
                        color: #718096;
                        font-size: 0.85em;
                    }}
                    .style-note {{
                        background-color: #ebf8ff;
                        padding: 20px;
                        margin: 15px 0;
                        border-radius: 8px;
                        border-left: 4px solid #4299e1;
                    }}
                    .style-note em {{
                        color: #2b6cb0;
                    }}
                    .used-npas {{
                        background-color: white;
                        padding: 30px;
                        border-radius: 12px;
                        margin-top: 60px;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
                    }}
                    .npa-list {{
                        list-style-type: none;
                        padding-left: 0;
                    }}
                    .npa-list li {{
                        padding: 12px 20px;
                        border-bottom: 1px solid #edf2f7;
                        color: #4a5568;
                    }}
                    .npa-list li:last-child {{
                        border-bottom: none;
                    }}
                    .overall-assessment {{
                        background-color: #f0f9ff;
                        padding: 30px;
                        border-radius: 12px;
                        margin: 40px 0;
                        border: 1px solid #bae6fd;
                        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
                    }}
                    .assessment-content {{
                        padding: 20px;
                        background-color: white;
                        border-radius: 8px;
                        line-height: 1.8;
                    }}
                    .assessment-content br {{
                        margin-bottom: 10px;
                    }}
                    em {{ font-style: italic; }}
                    strong {{ font-weight: bold; }}
                </style>
            </head>
            <body>
            {html_response}
            </body>
            </html>
            """)

        # Send the file
        from aiogram.types import FSInputFile
        document = FSInputFile(output_path, filename="–ê–Ω–∞–ª–∏–∑_–¢–ó.html")
        await callback_query.message.answer_document(
            document,
            caption="‚úÖ –ê–Ω–∞–ª–∏–∑ –¢–ó –∑–∞–≤–µ—Ä—à–µ–Ω. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ HTML —Ñ–∞–π–ª–µ."
        )
        await thinking_msg.delete()
        
    except Exception as e:
        logging.error(f"Error analyzing TZ text: {str(e)}")
        await thinking_msg.edit_text("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¢–ó. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


# –≠—Ö–æ-—Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ä–µ–∂–∏–º–∞ —á–∞—Ç–∞ –ø–æ –¢–ó
@router.message(BotStates.chat_tz)
async def handle_chat_message(message: Message, state: FSMContext):
    # Get PDFChat instance from state
    data = await state.get_data()
    pdf_chat = data.get('pdf_chat')
    
    if not pdf_chat:
        await message.reply("‚ùå –û—à–∏–±–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –¥–æ–∫—É–º–µ–Ω—Ç —á–µ—Ä–µ–∑ /upload_tech")
        return
    
    thinking_message = await message.reply("–î—É–º–∞—é...")  # Notify the user about processing

    try:
        # Get response from PDFChat
        response = pdf_chat.ask(message.text)
        await thinking_message.edit_text(response)  # Replace '–î—É–º–∞—é...' with the response
    except Exception as e:
        logging.error(f"Error in chat processing: {str(e)}")
        await thinking_message.edit_text("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∞—à–µ–≥–æ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑.")


# Function to filter documents based on –¢–ó keywords
def filter_documents_by_tz(json_file_path, keywords):
    """
    –§–∏–ª—å—Ç—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º, —Å–≤—è–∑–∞–Ω–Ω—ã–º —Å –¢–ó.

    :param json_file_path: –ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
    :param keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    :return: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¢–ó.
    """
    with open(json_file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    filtered_documents = []

    for document in data.get("–¥–æ–∫—É–º–µ–Ω—Ç—ã", []):
        file_name = document.get("–∏–º—è_—Ñ–∞–π–ª–∞", "").lower()
        if any(keyword.lower() in file_name for keyword in keywords):
            filtered_documents.append(document)

    return filtered_documents

# Example usage during –¢–ó upload
@router.message(BotStates.analytics_mode)
async def analyze_tz_documents(message: Message, state: FSMContext):
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ JSON –Ω–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –¢–ó.
    """
    await message.reply("–î—É–º–∞—é...")

    try:
        # Keywords related to the uploaded –¢–ó
        tz_keywords = ["–∫–æ–¥–µ–∫—Å", "–≥—Ä–∞–¥–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π", "–≤–æ–¥–Ω—ã–π", "–≥–æ—Å—Ç"]

        # Path to the JSON file
        json_path = "parsed_legal_documents (2).json"

        # Filter documents
        relevant_documents = filter_documents_by_tz(json_path, tz_keywords)

        # Respond with the results
        if relevant_documents:
            response = f"–ù–∞–π–¥–µ–Ω–æ {len(relevant_documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¢–ó:\n"
            response += "\n".join([doc.get("–∏–º—è_—Ñ–∞–π–ª–∞", "–ë–µ–∑ –∏–º–µ–Ω–∏") for doc in relevant_documents])
        else:
            response = "–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¢–ó."

        await message.reply(response)
    except Exception as e:
        logging.error(f"Error analyzing –¢–ó documents: {str(e)}")
        await message.reply("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


# Function to fetch recommendations for NPA using LangChain
def fetch_npa_recommendations(json_file_path, keywords):
    """
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç LangChain –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ JSON –∏ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ù–ü–ê.

    :param json_file_path: –ü—É—Ç—å –∫ JSON-—Ñ–∞–π–ª—É —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
    :param keywords: –°–ø–∏—Å–æ–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.
    :return: –°–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¢–ó.
    """
    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ –∏–∑ JSON
    graph = NetworkxEntityGraph()

    with open(json_file_path, "r", encoding="utf-8") as file:
        data = json.load(file)

    for document in data.get("–¥–æ–∫—É–º–µ–Ω—Ç—ã", []):
        file_name = document.get("–∏–º—è_—Ñ–∞–π–ª–∞", "").lower()
        if any(keyword.lower() in file_name for keywords in keywords):
            graph.add_node(file_name)

    # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    query = """
    MATCH (doc)
    WHERE """ + " OR ".join([f"doc.–∏–º—è_—Ñ–∞–π–ª–∞ CONTAINS '{keyword}'" for keyword in keywords]) + """
    RETURN doc.–∏–º—è_—Ñ–∞–π–ª–∞
    """

    # –í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–ø—Ä–æ—Å
    results = graph.query(query)

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    return [result["doc.–∏–º—è_—Ñ–∞–π–ª–∞"] for result in results]


@router.callback_query(F.data == "get_npa_recommendations")
async def get_npa_recommendations(callback_query: CallbackQuery):
    """
    –û–±—Ä–∞–±–æ—Ç—á–∏–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ –ù–ü–ê.
    """
    await callback_query.message.reply("–î—É–º–∞—é...")

    try:
        # Keywords related to the uploaded –¢–ó
        tz_keywords = ["–∫–æ–¥–µ–∫—Å", "–≥—Ä–∞–¥–æ—Å—Ç—Ä–æ–∏—Ç–µ–ª—å–Ω—ã–π", "–≤–æ–¥–Ω—ã–π", "–≥–æ—Å—Ç"]

        # Path to the JSON file
        json_path = "parsed_legal_documents (2).json"

        # Fetch recommendations using LangChain
        recommendations = fetch_npa_recommendations(json_path, tz_keywords)

        # Create inline keyboard with recommendations
        if recommendations:
            keyboard = InlineKeyboardMarkup(inline_keyboard=[
                [InlineKeyboardButton(text=doc, callback_data=f"npa_{i}")]
                for i, doc in enumerate(recommendations)
            ])
            await callback_query.message.reply("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ù–ü–ê:", reply_markup=keyboard)
        else:
            await callback_query.message.reply("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –¢–ó.")
    except Exception as e:
        logging.error(f"Error fetching NPA recommendations: {str(e)}")
        await callback_query.message.reply("‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")


# –§—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞
async def main():
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω –±–æ—Ç–∞ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–¥–∞–Ω–Ω—ã–π
    BOT_TOKEN = "8097160235:AAF0_dJuMn_jxRmZJDsMxUVIpsOGj6-MHRc"  # –ó–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ –≤–∞—à —Ç–æ–∫–µ–Ω

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    await init_db()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –±–æ—Ç–∞ –∏ –¥–∏—Å–ø–µ—Ç—á–µ—Ä
    bot = Bot(token=BOT_TOKEN)
    storage = MemoryStorage()
    dp = Dispatcher(storage=storage)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º PDFChat
    ollama_host = 'http://192.168.1.18:11434'
    pdf_chat = PDFChat(ollama_host=ollama_host)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDFChat –≤ data –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞—Ö
    dp.update.middleware(DatabaseMiddleware())
    dp.update.middleware(PDFChatMiddleware(pdf_chat))

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º middleware
    dp.update.middleware(DatabaseMiddleware())

    # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º —Ä–æ—É—Ç–µ—Ä
    dp.include_router(router)

    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞
    await dp.start_polling(bot)


class DatabaseMiddleware(BaseMiddleware):
    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        async with async_session_maker() as session:
            data["session"] = session
            result = await handler(event, data)
        return result


class PDFChatMiddleware(BaseMiddleware):
    def __init__(self, pdf_chat: PDFChat):
        self.pdf_chat = pdf_chat

    async def __call__(
        self,
        handler: Callable[[types.TelegramObject, Dict[str, Any]], Awaitable[Any]],
        event: types.TelegramObject,
        data: Dict[str, Any]
    ) -> Any:
        data["pdf_chat"] = self.pdf_chat
        return await handler(event, data)


if __name__ == '__main__':
    asyncio.run(main())